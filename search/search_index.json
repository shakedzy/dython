{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Dython","text":""},{"location":"#welcome","title":"Welcome!","text":"<p>Dython is a set of Data analysis tools in pYTHON 3.x, which can let you get more insights about your data.</p> <p>This library was designed with analysis usage in mind - meaning ease-of-use, functionality and readability are the core  values of this library. Production-grade performance, on the other hand, were not considered.</p> <p>Here are some cool things you can do with it:</p> <p>Given a dataset, Dython will automatically find which features are categorical and which are numerical, compute a relevant measure of association between each and every feature, and plot it all as an easy-to-read  heat-map. And all this is done with a single line:</p> <p><pre><code>from dython.nominal import associations\nassociations(data)\n</code></pre> The result:</p> <p></p> <p>Here's another thing - given a machine-learning multi-class model's predictions, you can easily display each class' ROC curve, AUC score and find the estimated-optimal thresholds - again, with a single line of code:</p> <p><pre><code>from dython.model_utils import metric_graph\n\nmetric_graph(y_true, y_pred, metric='roc')\n</code></pre> The result:</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Dython can be installed directly using <code>pip</code>: <pre><code>pip install dython\n</code></pre> Other installation options are available, see the installation page for more information.</p>"},{"location":"#examples","title":"Examples","text":"<p>See some usage examples of <code>nominal.associations</code> and <code>model_utils.roc_graph</code> on the examples page.</p>"},{"location":"#citing","title":"Citing","text":"<p>When using Dython, please cite it using this citation: <pre><code>@article{Zychlinski2025,\n    doi = {10.21105/joss.09174},\n    url = {https://doi.org/10.21105/joss.09174},\n    year = {2025},\n    publisher = {The Open Journal},\n    volume = {10},\n    number = {116},\n    pages = {9174},\n    author = {Shaked Zychlinski},\n    title = {dython: A Set of Analysis and Visualization Tools for Data and Variables in Python},\n    journal = {Journal of Open Source Software}\n }  \n</code></pre></p>"},{"location":"related_blogposts/","title":"Related Blogposts","text":"<p>Here are some blogposts I wrote, explaining and using some of the methods of Dython:</p> <ul> <li>Read more about the categorical tools on    The Search for Categorical Correlation</li> <li>Read more about using ROC graphs on    Hard ROC: Really Understanding &amp; Properly Using ROC and AUC</li> <li>Read more about KS Area Between Curves and when not to use ROC graphs (and other common metrics) on    The Metric System: How to Correctly Measure Your Model</li> </ul>"},{"location":"getting_started/examples/","title":"Examples","text":""},{"location":"getting_started/examples/#associations_iris_example","title":"<code>associations_iris_example()</code>","text":"<p>Plot an example of an associations heat-map of the Iris dataset features.  All features of this dataset are numerical (except for the target).</p> <p>Example code: <pre><code>import pandas as pd\nfrom sklearn import datasets\nfrom dython.nominal import associations\n\n# Load data \niris = datasets.load_iris()\n\n# Convert int classes to strings to allow associations \n# method to automatically recognize categorical columns\ntarget = ['C{}'.format(i) for i in iris.target]\n\n# Prepare data\nX = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ny = pd.DataFrame(data=target, columns=['target'])\ndf = pd.concat([X, y], axis=1)\n\n# Plot features associations\nassociations(df)\n</code></pre> Output:</p> <p></p>"},{"location":"getting_started/examples/#associations_mushrooms_example","title":"<code>associations_mushrooms_example()</code>","text":"<p>Plot an example of an associations heat-map of the UCI Mushrooms dataset features. All features of this dataset are categorical. This example will use Theil's U.</p> <p>Example code: <pre><code>import pandas as pd\nfrom dython.nominal import associations\n\n# Download and load data from UCI\ndf = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data')\ndf.columns = ['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n              'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n              'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type',\n              'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n\n# Plot features associations\nassociations(df, nom_nom_assoc='theil', figsize=(15, 15))\n</code></pre> Output:</p> <p></p>"},{"location":"getting_started/examples/#ks_abc_example","title":"<code>ks_abc_example()</code>","text":"<p>An example of KS Area Between Curve of a simple binary classifier trained over the Breast Cancer dataset.</p> <p>Example code: <pre><code>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom dython.model_utils import ks_abc\n\n# Load and split data\ndata = datasets.load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=.5, random_state=0)\n\n# Train model and predict\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)\n\n# Perform KS test and compute area between curves\nks_abc(y_test, y_pred[:,1])\n</code></pre></p> <p>Output:</p> <p></p>"},{"location":"getting_started/examples/#pr_graph_example","title":"<code>pr_graph_example()</code>","text":"<p>Plot an example Precision-Recall graph of an SVM model predictions over the Iris dataset.</p> <p>Example code:</p> <pre><code>import numpy as np\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom dython.model_utils import metric_graph\n\n# Load data\niris = datasets.load_iris()\nX = iris.data\ny = label_binarize(iris.target, classes=[0, 1, 2])\n\n# Add noisy features\nrandom_state = np.random.RandomState(4)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Train a model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n\n# Predict\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n\n# Plot ROC graphs\nmetric_graph(y_test, y_score, 'pr', class_names=iris.target_names)\n</code></pre> <p>Output:</p> <p></p>"},{"location":"getting_started/examples/#roc_graph_example","title":"<code>roc_graph_example()</code>","text":"<p>Plot an example ROC graph of an SVM model predictions over the Iris dataset.</p> <p>Based on <code>sklearn</code> examples  (as was seen on April 2018).</p> <p>Example code:</p> <pre><code>import numpy as np\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom dython.model_utils import metric_graph\n\n# Load data\niris = datasets.load_iris()\nX = iris.data\ny = label_binarize(iris.target, classes=[0, 1, 2])\n\n# Add noisy features\nrandom_state = np.random.RandomState(4)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Train a model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=0))\n\n# Predict\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n\n# Plot ROC graphs\nmetric_graph(y_test, y_score, 'roc', class_names=iris.target_names)\n</code></pre> <p>Output:</p> <p></p> <p>Note:</p> <p>Due to the nature of <code>np.random.RandomState</code> which is used in this  example, the output graph may vary from one machine to another.</p>"},{"location":"getting_started/examples/#split_hist_example","title":"<code>split_hist_example()</code>","text":"<p>Plot an example of split histogram of data from the breast-cancer dataset.</p> <p>While this example presents a numerical column split by a categorical one, categorical columns can also be used as the values, as well as numerical columns as the split criteria.</p> <p>Example code: <pre><code>import pandas as pd\nfrom sklearn import datasets\nfrom dython.data_utils import split_hist\n\n# Load data and convert to DataFrame\ndata = datasets.load_breast_cancer()\ndf = pd.DataFrame(data=data.data, columns=data.feature_names)\ndf['malignant'] = [not bool(x) for x in data.target]\n\n# Plot histogram\nsplit_hist(df, 'mean radius', split_by='malignant', bins=20, figsize=(15,7))\n</code></pre></p> <p>Output:</p> <p></p>"},{"location":"getting_started/installation/","title":"Installing Dython","text":""},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>The easiest way to install dython is using <code>pip install</code>:</p> <p><pre><code>pip install dython\n</code></pre> Or, via the <code>conda</code> package manager: <pre><code>conda install -c conda-forge dython\n</code></pre></p> <p>If you'd like to use the source code instead, you can install directly from it using any  of the following methods:</p> <ul> <li>Install source code using pip:  <pre><code>pip install git+https://github.com/shakedzy/dython.git`\n</code></pre></li> <li>Download the source code as a ZIP file</li> <li>Download the source code as a TAR ball</li> </ul>"},{"location":"modules/data_utils/","title":"data_utils","text":""},{"location":"modules/data_utils/#identify_columns_with_na","title":"<code>identify_columns_with_na</code>","text":"<p><code>identify_columns_with_na(dataset)</code></p> <p>Given a dataset, return columns names having NA values, sorted in descending order by their number of NAs.</p> <ul> <li><code>dataset</code> : <code>np.ndarray</code> / <code>pd.DataFrame</code></li> </ul> <p>Returns: A <code>pd.DataFrame</code> of two columns (<code>['column', 'na_count']</code>), consisting of only the names of columns with NA values, sorted by their number of NA values.</p> <p>Example: <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col1': ['a', np.nan, 'a', 'a'], 'col2': [3, np.nan, 2, np.nan], 'col3': [1., 2., 3., 4.]})\n&gt;&gt;&gt; identify_columns_with_na(df)\n  column  na_count\n1   col2         2\n0   col1         1\n</code></pre></p>"},{"location":"modules/data_utils/#identify_columns_by_type","title":"<code>identify_columns_by_type</code>","text":"<p><code>identify_columns_by_type(dataset, include)</code></p> <p>Given a dataset, identify columns of the types requested.</p> <ul> <li> <p><code>dataset</code> : <code>np.ndarray</code> / <code>pd.DataFrame</code></p> </li> <li> <p><code>include</code> : <code>list</code></p> <p>which column types to filter by.</p> </li> </ul> <p>Returns: list of categorical columns</p> <p>Example: <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col1': ['a', 'b', 'c', 'a'], 'col2': [3, 4, 2, 1], 'col3': [1., 2., 3., 4.]})\n&gt;&gt;&gt; identify_columns_by_type(df, include=['int64', 'float64'])\n['col2', 'col3']\n</code></pre></p>"},{"location":"modules/data_utils/#one_hot_encode","title":"<code>one_hot_encode</code>","text":"<p><code>one_hot_encode(arr, classes=None)</code></p> <p>One-hot encode a 1D array. Based on this StackOverflow answer.</p> <ul> <li> <p><code>arr</code> : array-like</p> <p>An array to be one-hot encoded. Must contain only non-negative integers</p> </li> <li> <p><code>classes</code> : <code>int</code> or <code>None</code></p> <p>number of classes. if None, max value of the array will be used</p> </li> </ul> <p>Returns: 2D one-hot encoded array</p> <p>Example: <pre><code>&gt;&gt;&gt; one_hot_encode([1,0,5])\n[[0. 1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1.]]\n</code></pre></p>"},{"location":"modules/data_utils/#split_hist","title":"<code>split_hist</code>","text":"<p><code>split_hist(dataset, values, split_by, title='', xlabel='', ylabel=None, figsize=None, legend='best', plot=True, **hist_kwargs)</code></p> <p>Plot a histogram of values from a given dataset, split by the values of a chosen column</p> <ul> <li> <p><code>dataset</code> : <code>pd.DataFrame</code></p> </li> <li> <p><code>values</code> : <code>string</code></p> <p>The column name of the values to be displayed in the histogram</p> </li> <li> <p><code>split_by</code> : <code>string</code></p> <p>The column name of the values to split the histogram by</p> </li> <li> <p><code>title</code> : <code>string</code> or <code>None</code>, default = ''</p> <p>The plot's title. If empty string, will be '{values} by {split_by}'</p> </li> <li> <p><code>xlabel</code>: <code>string</code> or <code>None</code>, default = ''</p> <p>x-axis label. If empty string, will be '{values}'</p> </li> <li> <p><code>ylabel</code>: <code>string</code> or <code>None</code>, default: <code>None</code></p> <p>y-axis label</p> </li> <li> <p><code>figsize</code>: (<code>int</code>,<code>int</code>) or <code>None</code>, default = <code>None</code></p> <p>A Matplotlib figure-size tuple. If <code>None</code>, falls back to Matplotlib's default.</p> </li> <li> <p><code>legend</code>: <code>string</code> or <code>None</code>, default = 'best'</p> <p>A Matplotlib legend location string. See Matplotlib documentation for possible options</p> </li> <li> <p><code>plot</code>: <code>Boolean</code>, default = True</p> <p>Plot the histogram</p> </li> <li> <p><code>hist_kwargs</code>: key-value pairs</p> <p>A key-value pairs to be passed to Matplotlib hist method. See Matplotlib documentation for possible options</p> </li> </ul> <p>Returns: A Matplotlib <code>Axe</code></p> <p>Example: See examples.</p>"},{"location":"modules/model_utils/","title":"model_utils","text":""},{"location":"modules/model_utils/#ks_abc","title":"<code>ks_abc</code>","text":"<p><code>ks_abc(y_true, y_pred, ax=None, figsize=None, colors=('darkorange', 'b'), title=None, xlim=(0.,1.), ylim=(0.,1.), fmt='.2f', lw=2, legend='best', plot=True, filename=None)</code></p> <p>Perform the Kolmogorov\u2013Smirnov test over the positive and negative distributions of a binary classifier, and compute the area between curves.</p> <p>The KS test plots the fraction of positives and negatives predicted correctly below each threshold. It then finds the optimal threshold, being the one enabling the best class separation.</p> <p>The area between curves allows a better insight into separation. The higher the area is (1 being the maximum), the more the positive and negative distributions' center-of-mass are closer to 1 and 0, respectively.</p> <p>Based on scikit-plot <code>plot_ks_statistic</code> method.</p> <ul> <li> <p><code>y_true</code> : array-like</p> <p>The true labels of the dataset</p> </li> <li> <p><code>y_pred</code> : array-like</p> <p>The probabilities predicted by a binary classifier</p> </li> <li> <p><code>ax</code> : matplotlib ax</p> <p>Default: None</p> <p>Matplotlib Axis on which the curves will be plotted</p> </li> <li> <p><code>figsize</code> : <code>(int,int)</code> or <code>None</code></p> <p>Default: None</p> <p>a Matplotlib figure-size tuple. If <code>None</code>, falls back to Matplotlib's default. Only used if <code>ax=None</code></p> </li> <li> <p><code>colors</code> : list of Matplotlib color strings</p> <p>Default: <code>('darkorange', 'b')</code></p> <p>List of colors to be used for the plotted curves</p> </li> <li> <p><code>title</code> : string or <code>None</code></p> <p>Default: None</p> <p>Plotted graph title. If <code>None</code>, default title is used</p> </li> <li> <p><code>xlim</code> : <code>(float, float)</code></p> <p>Default: (0.,1.)</p> <p>X-axis limits.</p> </li> <li> <p><code>ylim</code> : <code>(float,float)</code></p> <p>Default: (0.,1.)</p> <p>Y-axis limits.</p> </li> <li> <p><code>fmt</code> : <code>string</code></p> <p>Default: '.2f'</p> <p>String formatting of displayed numbers.</p> </li> <li> <p><code>lw</code> : <code>int</code></p> <p>Default: 2</p> <p>Line-width.</p> </li> <li> <p><code>legend</code>: <code>string</code> or <code>None</code></p> <p>Default: 'best'</p> <p>A Matplotlib legend location string. See Matplotlib documentation for possible options</p> </li> <li> <p><code>plot</code>: <code>Boolean</code>, default = True</p> <p>Plot the KS curves</p> </li> <li> <p><code>filename</code>: <code>string</code> or <code>None</code></p> <p>Default: None</p> <p>If not None, plot will be saved to the given file name.</p> </li> </ul> <p>Returns: A dictionary of the following keys:</p> <ul> <li> <p><code>abc</code>: area between curves</p> </li> <li> <p><code>ks_stat</code>: computed statistic of the KS test</p> </li> <li> <p><code>eopt</code>: estimated optimal threshold</p> </li> <li> <p><code>ax</code>: the ax used to plot the curves</p> </li> </ul> <p>Example: See examples.</p>"},{"location":"modules/model_utils/#metric_graph","title":"<code>metric_graph</code>","text":"<p><code>metric_graph(y_true, y_pred, metric, micro=True, macro=True, eoptimal_threshold=True, class_names=None, colors=None, ax=None, figsize=None, xlim=(0.,1.), ylim=(0.,1.02), lw=2, ls='-', ms=10, fmt='.2f', title=None, filename=None, force_multiclass=False)</code></p> <p>Plot a metric graph of predictor's results (including AUC scores), where each row of y_true and y_pred represent a single example.</p> <p>ROC: Plots true-positive rate as a function of the false-positive rate of the positive label in a binary classification, where \\(TPR = TP / (TP + FN)\\) and \\(FPR = FP / (FP + TN)\\). A naive algorithm will display a linear line going from (0,0) to (1,1), therefore having an area under-curve (AUC) of 0.5.</p> <p>Computes the estimated optimal threshold using two methods: * Geometric distance: Finding the closest point to the optimum at (0,1) using Euclidean distance * Youden's J: Maximizing \\(TPR - FPR\\) (corresponding to \\(Y - X\\))</p> <p>Precision-Recall: Plots precision as a function of recall of the positive label in a binary classification, where \\(Precision = TP / (TP + FP)\\) and \\(Recall = TP / (TP + FN)\\). A naive algorithm will display a horizontal linear line with precision of the ratio of positive examples in the dataset. Estimated optimal threshold is computed using Euclidean (geometric) distance.</p> <p>Based on scikit-learn examples (as was seen on April 2018):</p> <ul> <li> <p><code>y_true</code> : <code>list / NumPy ndarray</code></p> <p>The true classes of the predicted data. If only one or two columns exist, the data is treated as a binary classification (see input example below). If there are more than 2 columns, each column is considered a unique class, and a ROC graph and AUC score will be computed for each.</p> </li> <li> <p><code>y_pred</code> : <code>list / NumPy ndarray</code></p> <p>The predicted classes. Must have the same shape as <code>y_true</code>.</p> </li> <li> <p><code>metric</code> : <code>string</code></p> <p>The metric graph to plot. Currently supported: 'roc' for Receiver Operating Characteristic curve and 'pr' for Precision-Recall curve</p> </li> <li> <p><code>micro</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Whether to calculate a Micro graph (not applicable for binary cases)</p> </li> <li> <p><code>macro</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Whether to calculate a Macro graph (ROC metric only, not applicable for binary cases)</p> </li> <li> <p><code>eopt</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Whether to calculate and display the estimated-optimal threshold for each metric graph. For ROC curves, the estimated-optimal threshold is the closest computed threshold with (fpr,tpr) values closest to (0,1). For PR curves, it is the closest one to (1,1) (perfect recall and precision)</p> </li> <li> <p><code>class_names</code>: <code>list</code> or <code>string</code></p> <p>Default: None</p> <p>Names of the different classes. In a multi-class classification, the order must match the order of the classes probabilities in the input data. In a binary classification, can be a string or a list. If a list, only the last element will be used.</p> </li> <li> <p><code>colors</code> : list of Matplotlib color strings or <code>None</code></p> <p>Default: None</p> <p>List of colors to be used for the plotted curves. If <code>None</code>, falls back to a predefined default.</p> </li> <li> <p><code>ax</code> : matplotlib <code>ax</code></p> <p>Default: None</p> <p>Matplotlib Axis on which the curves will be plotted</p> </li> <li> <p><code>figsize</code> : <code>(int,int)</code> or <code>None</code></p> <p>Default: None</p> <p>A Matplotlib figure-size tuple. If <code>None</code>, falls back to Matplotlib's default. Only used if <code>ax=None</code>.</p> </li> <li> <p><code>xlim</code> : <code>(float, float)</code></p> <p>Default: (0.,1.)</p> <p>X-axis limits.</p> </li> <li> <p><code>ylim</code> : <code>(float,float)</code></p> <p>Default: (0.,1.02)</p> <p>Y-axis limits.</p> </li> <li> <p><code>lw</code> : <code>int</code></p> <p>Default: 2</p> <p>Line-width.</p> </li> <li> <p><code>ls</code> : <code>string</code></p> <p>Default: '-'</p> <p>Matplotlib line-style string</p> </li> <li> <p><code>ms</code> : <code>int</code></p> <p>Default: 10</p> <p>Marker-size.</p> </li> <li> <p><code>fmt</code> : <code>string</code></p> <p>Default: '.2f'</p> <p>String formatting of displayed AUC and threshold numbers.</p> </li> <li> <p><code>legend</code>: <code>string</code> or <code>None</code></p> <p>Default: 'best'</p> <p>A Matplotlib legend location string. See Matplotlib documentation for possible options</p> </li> <li> <p><code>plot</code>: <code>Boolean</code>, default = True</p> <p>Plot the histogram</p> </li> <li> <p><code>title</code>: <code>string</code> or <code>None</code></p> <p>Default: None</p> <p>Plotted graph title. If None, default title is used.</p> </li> <li> <p><code>filename</code>: <code>string</code> or <code>None</code></p> <p>Default: None</p> <p>If not None, plot will be saved to the given file name.</p> </li> <li> <p><code>force_multiclass</code>: <code>Boolean</code></p> <p>Default: False</p> <p>Only applicable if <code>y_true</code> and <code>y_pred</code> have two columns. If so, consider the data as a multiclass data rather than binary (useful when plotting curves of different models one against the other)</p> </li> </ul> <p>Returns: </p> <p>Return value changed</p> <p>The keys of the returned <code>dict</code> of this function changed on version 0.7.12 </p> <p>A dictionary with these keys: - <code>ax</code>: the Matplotlib plot axis - <code>metrics</code>: each key is a class name from the list of provided classes.,               Per each class, another dict exists with AUC results              and measurement methods results.              AUC key holds both the measured area-under-curve (under <code>val</code>)              and the AUC of a random-guess classifier (under <code>naive</code>) for              comparison.              Each measurement method key contains three values: <code>x</code>, <code>y</code>, <code>val</code>,              corresponding to the (x,y) coordinates on the metric graph of the              threshold, and its value.              If only one class exists, then the measurements method keys and AUC              will be directly under <code>metrics</code>.</p> <p>Example: See examples.</p> <p>Binary Classification Input Example: Consider a data-set of two data-points where the true class of the first line is class 0, which was predicted with a probability of 0.6, and the second line's true class is 1, with predicted probability of 0.8. <pre><code># First option:\n&gt;&gt;&gt; metric_graph(y_true=[0,1], y_pred=[0.6,0.8], metric='roc')\n# Second option:\n&gt;&gt;&gt; metric_graph(y_true=[[1,0],[0,1]], y_pred=[[0.6,0.4],[0.2,0.8]], metric='roc')\n# Both yield the same result\n</code></pre></p>"},{"location":"modules/model_utils/#random_forest_feature_importance","title":"<code>random_forest_feature_importance</code>","text":"<p><code>random_forest_feature_importance(forest, features, precision=4)</code></p> <p>Given a trained <code>sklearn.ensemble.RandomForestClassifier</code>, plot the different features based on their importance according to the classifier, from the most important to the least.</p> <ul> <li> <p><code>forest</code> : <code>sklearn.ensemble.RandomForestClassifier</code></p> <p>A trained <code>RandomForestClassifier</code></p> </li> <li> <p><code>features</code> : <code>list</code></p> <p>A list of the names of the features the classifier was trained on, ordered by the same order the appeared in the training data</p> </li> <li> <p><code>precision</code> : <code>int</code></p> <p>Default: 4</p> <p>Precision of feature importance.</p> </li> </ul>"},{"location":"modules/nominal/","title":"nominal","text":""},{"location":"modules/nominal/#associations","title":"<code>associations</code>","text":"<p><code>associations(dataset, nominal_columns='auto', numerical_columns=None, mark_columns=False,nom_nom_assoc='cramer', num_num_assoc='pearson', nom_num_assoc='correlation_ratio', symmetric_nom_nom=True, symmetric_num_num=True, display_rows='all', display_columns='all', hide_rows=None, hide_columns=None, cramers_v_bias_correction=True, nan_strategy=_REPLACE, nan_replace_value=_DEFAULT_REPLACE_VALUE, ax=None, figsize=None, annot=True, fmt='.2f', cmap=None, sv_color='silver', cbar=True, vmax=1.0, vmin=None, plot=True, compute_only=False, clustering=False, title=None, filename=None, multiprocessing=False, max_cpu_cores=None)</code></p> <p>Calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using:  * Pearson's R for continuous-continuous cases  * Correlation Ratio for categorical-continuous cases  * Cramer's V or Theil's U for categorical-categorical cases</p> <ul> <li> <p><code>dataset</code> : <code>NumPy ndarray / Pandas DataFrame</code></p> <p>The data-set for which the features' correlation is computed</p> </li> <li> <p><code>nominal_columns</code> : <code>string / list / NumPy ndarray</code></p> <p>Default: 'auto'</p> <p>Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical. Only used if <code>numerical_columns</code> is <code>None</code>.</p> </li> <li> <p><code>numerical_columns</code> : <code>string / list / NumPy ndarray</code></p> <p>Default: None</p> <p>To be used instead of <code>nominal_columns</code>. Names of columns of the data-set which hold numerical values. Can also be the string 'all' to state that all columns are numerical (equivalent to <code>nominal_columns=None</code>) or 'auto' to try to identify numerical columns (equivalent to <code>nominal_columns=auto</code>). If <code>None</code>, <code>nominal_columns</code> is used.</p> </li> <li> <p><code>mark_columns</code> : <code>Boolean</code></p> <p>Default: False</p> <p>if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on their type (nominal or continuous), as provided by nominal_columns</p> </li> <li> <p><code>nom_nom_assoc</code> : <code>callable / string</code></p> <p>Default: 'cramer'</p> <p>Method signature change</p> <p>This replaces the <code>theil_u</code> flag which was used till version 0.6.6.</p> <p>If callable, a function which recieves two <code>pd.Series</code> and returns a single number.</p> <p>If string, name of nominal-nominal (categorical-categorical) association to use:</p> <ul> <li> <p><code>cramer</code>: Cramer's V</p> </li> <li> <p><code>theil</code>: Theil's U. When selected, heat-map columns are the provided information (meaning: \\(U = U(row|col)\\))</p> </li> </ul> </li> <li> <p><code>num_num_assoc</code> : <code>callable / string</code></p> <p>Default: 'pearson'</p> <p>If callable, a function which recieves two <code>pd.Series</code> and returns a single number.</p> <p>If string, name of numerical-numerical association to use:</p> <ul> <li> <p><code>pearson</code>: Pearson's R</p> </li> <li> <p><code>spearman</code>: Spearman's R</p> </li> <li> <p><code>kendall</code>: Kendall's Tau</p> </li> </ul> </li> <li> <p><code>nom_num_assoc</code> : <code>callable / string</code></p> <p>Default: 'correlation_ratio'</p> <p>If callable, a function which recieves two <code>pd.Series</code> and returns a single number.</p> <p>If string, name of nominal-numerical association to use:</p> <ul> <li><code>correlation_ratio</code>: correlation ratio</li> </ul> </li> <li> <p><code>symmetric_nom_nom</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Relevant only if <code>nom_nom_assoc</code> is a callable. If so, declare whether the function is symmetric (\\(f(x,y) = f(y,x)\\)). If False, heat-map values should be interpreted as \\(f(row,col)\\).</p> </li> <li> <p><code>symmetric_num_num</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Relevant only if <code>num_num_assoc</code> is a callable. If so, declare whether the function is symmetric (\\(f(x,y) = f(y,x)\\)). If False, heat-map values should be interpreted as \\(f(row,col)\\). </p> </li> <li> <p><code>display_rows</code> : <code>list / string</code></p> <p>Default: 'all'</p> <p>Choose which of the dataset's features will be displyed in the output's correlations table rows. If string, can either be a single feature's name or 'all'. Only used if <code>hide_rows</code> is <code>None</code>.</p> </li> <li> <p><code>display_columns</code> : <code>list / string</code></p> <p>Default: 'all'</p> <p>Choose which of the dataset's features will be displyed in the output's correlations table columns. If string, can either be a single feature's name or 'all'. Only used if <code>hide_columns</code> is <code>None</code>.</p> </li> <li> <p><code>hide_rows</code> : <code>list / string</code></p> <p>Default: None</p> <p>choose which of the dataset's features will not be displyed in the output's correlations table rows. If string, must be a single feature's name. If <code>None</code>, <code>display_rows</code> is used.</p> </li> <li> <p><code>hide_columns</code> : <code>list / string</code></p> <p>Default: None</p> <p>choose which of the dataset's features will not be displyed in the output's correlations table columns. If string, must be a single feature's name. If <code>None</code>, <code>display_columns</code> is used.</p> </li> <li> <p><code>cramers_v_bias_correction</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Method signature change</p> <p>This replaces the <code>bias_correction</code> flag which was used till version 0.6.6.</p> <p>Use bias correction for Cramer's V from Bergsma and Wicher, Journal of the Korean  Statistical Society 42 (2013): 323-328.</p> </li> <li> <p><code>nan_strategy</code> : <code>string</code></p> <p>Default: 'replace'</p> <p>How to handle missing values: can be either <code>'drop_samples'</code> to remove samples with missing values, <code>'drop_features'</code> to remove features (columns) with missing values, <code>'replace'</code> to replace all missing values with the <code>nan_replace_value</code>, or <code>'drop_sample_pairs'</code> to drop each pair of missing observables separately before calculating the corresponding coefficient. Missing values are <code>None</code> and <code>np.nan</code>.</p> </li> <li> <p><code>nan_replace_value</code> : <code>any</code></p> <p>Default: 0.0</p> <p>The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'</p> </li> <li> <p><code>ax</code> : matplotlib <code>Axe</code></p> <p>Default: None</p> <p>Matplotlib Axis on which the heat-map will be plotted</p> </li> <li> <p><code>figsize</code> : <code>(float, float)</code> or <code>None</code></p> <p>Default: None</p> <p>A Matplotlib figure-size tuple. If <code>None</code>, will attempt to set the size automatically.  Only used if <code>ax=None</code>.</p> </li> <li> <p><code>annot</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Plot number annotations on the heat-map</p> </li> <li> <p><code>fmt</code> : <code>string</code></p> <p>Default: '.2f'</p> <p>String formatting of annotations</p> </li> <li> <p><code>cmap</code> : Matplotlib colormap or <code>None</code></p> <p>Default: None</p> <p>A colormap to be used for the heat-map. If None, falls back to Seaborn's heat-map default</p> </li> <li> <p><code>sv_color</code> : <code>string</code></p> <p>Default: 'silver'</p> <p>A Matplotlib color. The color to be used when displaying single-value features over the heat-map</p> </li> <li> <p><code>cbar</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Display heat-map's color-bar</p> </li> <li> <p><code>vmax</code> : <code>float</code></p> <p>Default: 1.0</p> <p>Set heat-map <code>vmax</code> option</p> </li> <li> <p><code>vmin</code> : <code>float</code> or <code>None</code></p> <p>Default: None</p> <p>Set heat-map <code>vmin</code> option. If set to <code>None</code>, <code>vmin</code> will be chosen automatically between 0 and -1.0, depending on the types of associations used (-1.0 if Pearson's R is used, 0 otherwise)</p> </li> <li> <p><code>plot</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Plot a heat-map of the correlation matrix. If False, heat-map will still be drawn, but not shown. The heat-map's <code>ax</code> is part of this function's output.</p> </li> <li> <p><code>compute_only</code> : <code>Boolean</code></p> <p>Default: False</p> <p>Use this flag only if you have no need of the plotting at all. This skips the entire plotting mechanism (similar to the old <code>compute_associations</code> method).</p> </li> <li> <p><code>clustering</code> : <code>Boolean</code></p> <p>Default: False</p> <p>If True, the computed associations will be sorted into groups by similar correlations</p> </li> <li> <p><code>title</code>: <code>string</code> or <code>None</code></p> <p>Default: None</p> <p>Plotted graph title.</p> </li> <li> <p><code>filename</code>: <code>string</code> or <code>None</code></p> <p>Default: None</p> <p>If not None, plot will be saved to the given file name.</p> </li> <li> <p><code>multiprocessing</code>: <code>Boolean</code></p> <p>Default: False</p> <p>If True, use multiprocessing to speed up computations. If None, falls back to single core computation</p> </li> <li> <p><code>max_cpu_cores</code>: <code>int</code> or <code>None</code> </p> <p>Default: <code>None</code></p> <p>If not <code>None</code>, <code>ProcessPoolExecutor</code> will use the given number of CPU cores</p> </li> </ul> <p>Returns: A dictionary with the following keys:</p> <ul> <li><code>corr</code>: A DataFrame of the correlation/strength-of-association between all features</li> <li><code>ax</code>: A Matplotlib <code>Axe</code></li> </ul> <p>Example: See examples.</p>"},{"location":"modules/nominal/#cluster_correlations","title":"<code>cluster_correlations</code>","text":"<p><code>cluster_correlations(corr_mat, indexes=None)</code></p> <p>Apply agglomerative clustering in order to sort a correlation matrix. Based on this clustering example.</p> <ul> <li> <p><code>corr_mat</code> : <code>Pandas DataFrame</code></p> <p>A correlation matrix (as output from <code>associations</code>)</p> </li> <li> <p><code>indexes</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of cluster indexes for sorting. If not present, a clustering is performed.</p> </li> </ul> <p>Returns:</p> <ul> <li>a sorted correlation matrix (<code>pd.DataFrame</code>)</li> <li>cluster indexes based on the original dataset (<code>list</code>)</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; assoc = associations(\n  customers,\n  plot=False\n)\n&gt;&gt;&gt; correlations = assoc['corr']\n&gt;&gt;&gt; correlations, _ = cluster_correlations(correlations)\n</code></pre></p>"},{"location":"modules/nominal/#compute_associations","title":"<code>compute_associations</code>","text":"<p>Deprecated</p> <p><code>compute_associations</code> was deprecated and removed. Use <code>associations(compute_only=True)['corr']</code>.</p>"},{"location":"modules/nominal/#conditional_entropy","title":"<code>conditional_entropy</code>","text":"<p><code>conditional_entropy(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE, log_base=math.e)</code></p> <p>Given measurements <code>x</code> and <code>y</code> of random variables \\(X\\) and \\(Y\\), calculates the conditional entropy of \\(X\\) given \\(Y\\):</p> \\[ S(X|Y) = - \\sum_{x,y} p(x,y) \\log\\frac{p(x,y)}{p(y)} \\] <p>Read more on Wikipedia.</p> <ul> <li> <p><code>x</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of measurements</p> </li> <li> <p><code>y</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of measurements</p> </li> <li> <p><code>nan_strategy</code> : <code>string</code></p> <p>Default: 'replace'</p> <p>How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan.</p> </li> <li> <p><code>nan_replace_value</code> : <code>any</code></p> <p>Default: 0.0</p> <p>The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'.</p> </li> <li> <p><code>log_base</code> : <code>float</code></p> <p>Default: <code>math.e</code></p> <p>Specifying base for calculating entropy.</p> </li> </ul> <p>Returns: <code>float</code></p>"},{"location":"modules/nominal/#correlation_ratio","title":"<code>correlation_ratio</code>","text":"<p><code>correlation_ratio(categories, measurements, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE)</code></p> <p>Calculates the Correlation Ratio (\\(\\eta\\)) for categorical-continuous association:</p> \\[ \\eta = \\sqrt{\\frac{\\sum_x{n_x (\\bar{y}_x - \\bar{y})^2}}{\\sum_{x,i}{(y_{xi}-\\bar{y})^2}}} \\] <p>where \\(n_x\\) is the number of observations in category \\(x\\), and we define:</p> \\[\\bar{y}_x = \\frac{\\sum_i{y_{xi}}}{n_x} , \\bar{y} = \\frac{\\sum_i{n_x \\bar{y}_x}}{\\sum_x{n_x}}\\] <p>Answers the question - given a continuous value of a measurement, is it possible to know which category is it associated with? Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means a category can be determined with absolute certainty. Read more on Wikipedia.</p> <ul> <li> <p><code>categories</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of categorical measurements</p> </li> <li> <p><code>measurements</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of continuous measurements</p> </li> <li> <p><code>nan_strategy</code> : <code>string</code></p> <p>Default: 'replace'</p> <p>How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan.</p> </li> <li> <p><code>nan_replace_value</code> : <code>any</code></p> <p>Default: 0.0</p> <p>The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'.</p> </li> </ul> <p>Returns: float in the range of [0,1]</p>"},{"location":"modules/nominal/#cramers_v","title":"<code>cramers_v</code>","text":"<p><code>cramers_v(x, y, bias_correction=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE)</code></p> <p>Calculates Cramer's V statistic for categorical-categorical association. This is a symmetric coefficient: \\(V(x,y) = V(y,x)\\). Read more on Wikipedia.</p> <p>Original function taken from this answer on StackOverflow.</p> <p>Cramer's V limitations when applied on skewed or small datasets</p> <p>As the Cramer's V measure of association depends directly on the counts of each samples-pair in the data, it tends to be suboptimal when applied on skewed or small datasets.</p> <p>Consider each of the following cases, where we would expect Cramer's V to reach a high value, yet this only happens in the first scenario:</p> <pre><code>&gt;&gt;&gt; x = ['a'] * 400 + ['b'] * 100\n&gt;&gt;&gt; y = ['X'] * 400 + ['Y'] * 100\n&gt;&gt;&gt; cramers_v(x,y)\n0.9937374102534072\n\n# skewed dataset\n&gt;&gt;&gt; x = ['a'] * 500 + ['b'] * 1\n&gt;&gt;&gt; y = ['X'] * 500 + ['Y'] * 1\n&gt;&gt;&gt; cramers_v(x,y)\n0.4974896903293253\n\n# very small dataset\n&gt;&gt;&gt; x = ['a'] * 4 + ['b'] * 1\n&gt;&gt;&gt; y = ['X'] * 4 + ['Y'] * 1\n&gt;&gt;&gt; cramers_v(x,y)\n0.0\n</code></pre> <ul> <li> <p><code>x</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of categorical measurements</p> </li> <li> <p><code>y</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of categorical measurements</p> </li> <li> <p><code>bias_correction</code> : <code>Boolean</code></p> <p>Default: True</p> <p>Use bias correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013): 323-328.</p> </li> <li> <p><code>nan_strategy</code> : <code>string</code></p> <p>Default: 'replace'</p> <p>How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan.</p> </li> <li> <p><code>nan_replace_value</code> : <code>any</code></p> <p>Default: 0.0</p> <p>The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'.</p> </li> </ul> <p>Returns: float in the range of [0,1]</p>"},{"location":"modules/nominal/#identify_nominal_columns","title":"<code>identify_nominal_columns</code>","text":"<p><code>identify_nominal_columns(dataset)</code></p> <p>Given a dataset, identify categorical columns. This is used internally in <code>associations</code> and <code>numerical_encoding</code>, but can also be used directly.</p> <p>Note:</p> <p>This is a shortcut for <code>data_utils.identify_columns_by_type(dataset, include=['object', 'category'])</code></p> <ul> <li><code>dataset</code> : <code>np.ndarray</code> / <code>pd.DataFrame</code></li> </ul> <p>Returns: list of categorical columns</p> <p>Example: <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col1': ['a', 'b', 'c', 'a'], 'col2': [3, 4, 2, 1]})\n&gt;&gt;&gt; identify_nominal_columns(df)\n['col1']\n</code></pre></p>"},{"location":"modules/nominal/#identify_numeric_columns","title":"<code>identify_numeric_columns</code>","text":"<p><code>identify_numeric_columns(dataset)</code></p> <p>Given a dataset, identify numeric columns.</p> <p>Note:</p> <p>This is a shortcut for <code>data_utils.identify_columns_by_type(dataset, include=['int64', 'float64'])</code></p> <ul> <li><code>dataset</code> : <code>np.ndarray</code> / <code>pd.DataFrame</code></li> </ul> <p>Returns: list of numerical columns</p> <p>Example: <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col1': ['a', 'b', 'c', 'a'], 'col2': [3, 4, 2, 1], 'col3': [1., 2., 3., 4.]})\n&gt;&gt;&gt; identify_numeric_columns(df)\n['col2', 'col3']\n</code></pre></p>"},{"location":"modules/nominal/#numerical_encoding","title":"<code>numerical_encoding</code>","text":"<p><code>numerical_encoding(dataset, nominal_columns='auto', drop_single_label=False, drop_fact_dict=True, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE)</code></p> <p>Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set, using the following logic:</p> <ul> <li> <p>categorical with only a single value will be marked as zero (or dropped, if requested)</p> </li> <li> <p>categorical with two values will be replaced with the result of Pandas <code>factorize</code></p> </li> <li> <p>categorical with more than two values will be replaced with the result of Pandas <code>get_dummies</code></p> </li> <li> <p>numerical columns will not be modified</p> </li> <li> <p><code>dataset</code> : <code>NumPy ndarray / Pandas DataFrame</code></p> <p>The data-set to encode</p> </li> <li> <p><code>nominal_columns</code> : <code>sequence / string</code></p> <p>Default: 'auto'</p> <p>Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all columns are categorical, 'auto' (default) to identify nominal columns automatically, or None to state none are categorical (nothing happens)</p> </li> <li> <p><code>drop_single_label</code> : <code>Boolean</code></p> <p>Default: False</p> <p>If True, nominal columns with a only a single value will be dropped.</p> </li> <li> <p><code>drop_fact_dict</code> : <code>Boolean</code></p> <p>Default: True</p> <p>If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of the DataFrame and the dictionary of the binary factorization (originating from pd.factorize)</p> </li> <li> <p><code>nan_strategy</code> : <code>string</code></p> <p>Default: 'replace'</p> <p>How to handle missing values: can be either 'drop_samples' to remove samples with missing values, 'drop_features' to remove features (columns) with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan.</p> </li> <li> <p><code>nan_replace_value</code> : <code>any</code></p> <p>Default: 0.0</p> <p>The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'</p> </li> </ul> <p>Returns: <code>pd.DataFrame</code> or <code>(pd.DataFrame, dict)</code>. If <code>drop_fact_dict</code> is True, returns the encoded DataFrame. else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the value is the original labels, as supplied by Pandas <code>factorize</code>. Will be empty if no two-value columns are present in the data-set</p>"},{"location":"modules/nominal/#replot_last_associations","title":"<code>replot_last_associations</code>","text":"<p><code>replot_last_associations(ax=None, figsize=None, annot=None, fmt=None, cmap=None, sv_color=None, cbar=None, vmax=None, vmin=None, plot=True, title=None, filename=None)</code></p> <p>Re-plot last computed associations heat-map. This method performs no new computations, but only allows to change the visual output of the last computed heat-map.</p> <ul> <li> <p><code>ax</code> : matplotlib <code>Axe</code> </p> <p>Default: <code>None</code></p> <p>Matplotlib Axis on which the heat-map will be plotted</p> </li> <li> <p><code>figsize</code> : <code>(int,int)</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>A Matplotlib figure-size tuple. If <code>None</code>, uses the last <code>associations</code> call value. Only used if <code>ax=None</code>.</p> </li> <li> <p><code>annot</code> : <code>Boolean</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>Plot number annotations on the heat-map. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>fmt</code> : <code>string</code></p> <p>Default: <code>None</code></p> <p>String formatting of annotations. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>cmap</code> : Matplotlib <code>colormap</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>A colormap to be used for the heat-map. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>sv_color</code> : <code>string</code></p> <p>Default: <code>None</code></p> <p>A Matplotlib color. The color to be used when displaying single-value. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>cbar</code> : <code>Boolean</code>or <code>None</code></p> <p>Default: <code>None</code></p> <p>Display heat-map's color-bar. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>vmax</code> : <code>float</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>Set heat-map <code>vmax</code> option. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>vmin</code> : <code>float</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>Set heat-map <code>vmin</code> option. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>plot</code> : <code>Boolean</code></p> <p>Default: <code>True</code></p> <p>Plot a heat-map of the correlation matrix. If False, plotting still happens, but the heat-map will not be displayed.</p> </li> <li> <p><code>title</code> : <code>string</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>Plotted graph title. If <code>None</code>, uses the last <code>associations</code> call value.</p> </li> <li> <p><code>filename</code> : <code>string</code> or <code>None</code></p> <p>Default: <code>None</code></p> <p>If not <code>None</code>, plot will be saved to the given file name. Note: in order to avoid accidental file overwrites, the last <code>associations</code> call value is never used, and when filename is set to None, no writing to file occurs.</p> </li> </ul> <p>Returns: A Matplotlib <code>Axe</code></p>"},{"location":"modules/nominal/#theils_u","title":"<code>theils_u</code>","text":"<p><code>theils_u(x, y, nan_strategy=REPLACE, nan_replace_value=DEFAULT_REPLACE_VALUE)</code></p> <p>Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association, defined as:</p> \\[ U(X|Y) = \\frac{S(X) - S(X|Y)}{S(X)} \\] <p>where \\(S(X)\\) is the entropy of \\(X\\) and \\(S(X|Y)\\) is the conditional entropy of \\(X\\) given \\(Y\\).</p> <p>This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about x, and 1 means y provides full information about x. This is an asymmetric coefficient: \\(U(x,y) \\neq U(y,x)\\). Read more on Wikipedia.</p> <ul> <li> <p><code>x</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of categorical measurements</p> </li> <li> <p><code>y</code> : <code>list / NumPy ndarray / Pandas Series</code></p> <p>A sequence of categorical measurements</p> </li> <li> <p><code>nan_strategy</code> : <code>string</code></p> <p>Default: 'replace'</p> <p>How to handle missing values: can be either 'drop' to remove samples with missing values, or 'replace' to replace all missing values with the nan_replace_value. Missing values are None and np.nan.</p> </li> <li> <p><code>nan_replace_value</code> : <code>any</code></p> <p>Default: 0.0</p> <p>The value used to replace missing values with. Only applicable when nan_strategy is set to 'replace'.</p> </li> </ul> <p>Returns: float in the range of [0,1]</p>"},{"location":"modules/sampling/","title":"sampling","text":""},{"location":"modules/sampling/#boltzmann_sampling","title":"<code>boltzmann_sampling</code>","text":"<p><code>boltzmann_sampling(numbers, k=1, with_replacement=False)</code></p> <p>Return k numbers from a boltzmann-sampling over the supplied numbers</p> <ul> <li> <p><code>numbers</code> : <code>List or np.ndarray</code></p> <p>numbers to sample</p> </li> <li> <p><code>k</code> : <code>int</code></p> <p>Default: 1</p> <p>How many numbers to sample. Choosing <code>k=None</code> will yield a single number</p> </li> <li> <p><code>with_replacement</code> : <code>Boolean</code> <code>Default: False</code></p> <p>Allow replacement or not</p> </li> </ul> <p>Returns: <code>list</code>, <code>np.ndarray</code> or a single number (depending on the input)</p>"},{"location":"modules/sampling/#weighted_sampling","title":"<code>weighted_sampling</code>","text":"<p><code>weighted_sampling(numbers, k=1, with_replacement=False)</code></p> <p>Return k numbers from a weighted-sampling over the supplied numbers</p> <ul> <li> <p><code>numbers</code> : <code>List or np.ndarray</code></p> <p>numbers to sample</p> </li> <li> <p><code>k</code> : <code>int</code></p> <p>Default: 1</p> <p>How many numbers to sample. Choosing <code>k=None</code> will yield a single number</p> </li> <li> <p><code>with_replacement</code> : <code>Boolean</code> </p> <p>Default: False</p> <p>Allow replacement or not</p> </li> </ul> <p>Returns: <code>list</code>, <code>np.ndarray</code> or a single number (depending on the input)</p>"}]}